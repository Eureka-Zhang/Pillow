# -*- coding: utf-8 -*-
"""
Real-time Snore Detection for Orange Pi 5 Plus (RK3588)
ä»è¯ç­’å®æ—¶é‡‡é›†éŸ³é¢‘ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£è¿›è¡Œæ‰“é¼¾æ£€æµ‹å’Œç¡å§¿è¯†åˆ«

æ”¯æŒæ¨ç†åç«¯:
- PyTorch CPU (é»˜è®¤)
- RKNN NPU (RK3588 åŠ é€Ÿï¼Œéœ€å®‰è£… rknn-toolkit-lite2)

Melé¢‘è°±å¤„ç†æµç¨‹ (ä¸è®­ç»ƒæ—¶ä¸€è‡´):
1. é‡é‡‡æ ·åˆ° 16kHz
2. æˆªå–/è¡¥é›¶åˆ° 3 ç§’
3. é«˜é¢‘é¢„åŠ é‡ (preemphasis)
4. è®¡ç®— Mel é¢‘è°±å›¾ (n_mels=80, n_fft=400, hop_length=160, hammingçª—)
5. è½¬æ¢ä¸ºå¯¹æ•°åˆ»åº¦ (power_to_db)
6. æ ‡å‡†åŒ– (mean=0, std=1)
7. æ¨ç†å‰å†æ¬¡æ ‡å‡†åŒ– (ä¸ train_MLT.py ä¸­ __getitem__ ä¸€è‡´)

Usage:
    # PyTorch CPU æ¨ç†
    python3 realtime_inference.py --model_path ./mlt_best.pth
    
    # RKNN NPU æ¨ç† (æ¨èï¼Œé€Ÿåº¦æ›´å¿«)
    python3 realtime_inference.py --model_path ./model.rknn --backend rknn
    
    # åˆ—å‡ºéŸ³é¢‘è®¾å¤‡
    python3 realtime_inference.py --list_devices
    
    # æµ‹è¯•éº¦å…‹é£
    python3 realtime_inference.py --test_audio
"""

import os
import sys
import time
import argparse
import threading
import queue
from datetime import datetime

import numpy as np
import librosa
import sounddevice as sd

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))


# =====================
# é…ç½®å‚æ•° (ä¸ MEL_preprocess.py çš„ "Mel" é…ç½®å®Œå…¨ä¸€è‡´)
# =====================
class Config:
    # Audio parameters
    SAMPLE_RATE = 16000          # é‡‡æ ·ç‡ 16kHz
    WINDOW_DURATION = 3.0        # çª—å£é•¿åº¦ 3ç§’
    SLIDE_INTERVAL = 1.0         # æ»‘åŠ¨é—´éš” 1ç§’
    
    # Mel spectrogram parameters (ä¸ MEL_preprocess.py "Mel" preset ä¸€è‡´)
    N_MELS = 80                  # Melé¢‘å¸¦æ•°
    N_FFT = 400                  # FFTçª—å£ (25ms @ 16kHz)
    HOP_LENGTH = 160             # è·³å¸§é•¿åº¦ (10ms @ 16kHz)
    WIN_LENGTH = 400             # çª—é•¿ = n_fft
    WINDOW = "hamming"           # çª—å‡½æ•°
    POWER = 2.0                  # åŠŸç‡è°±
    
    # é¢„å¤„ç†é€‰é¡¹
    PREEMPHASIS = True           # é«˜é¢‘é¢„åŠ é‡
    NORMALIZE = True             # æ ‡å‡†åŒ– (ç¬¬ä¸€æ¬¡ï¼Œåœ¨melè®¡ç®—å)
    
    # Model parameters
    SNORE_CLASSES = 2
    POSTURE_CLASSES = 5          # 5ç±»ç¡å§¿ (è®­ç»ƒæ—¶æ’é™¤äº†ç¬¬6ç±»)
    
    # Detection thresholds
    SNORE_THRESHOLD = 0.5        # æ‰“é¼¾åˆ¤å®šé˜ˆå€¼


# =====================
# Melé¢‘è°±æå– (ä¸ MEL_preprocess.py çš„ preprocess_audio_mel å®Œå…¨ä¸€è‡´)
# =====================
def extract_mel_spectrogram(audio, config=Config):
    """
    å°†éŸ³é¢‘è½¬æ¢ä¸ºMelé¢‘è°±å›¾
    å®Œå…¨å¤åˆ» MEL_preprocess.py ä¸­çš„å¤„ç†æµç¨‹
    
    Args:
        audio: np.ndarray, éŸ³é¢‘æ•°æ® (å·²ç»æ˜¯ config.SAMPLE_RATE é‡‡æ ·ç‡)
        config: é…ç½®å¯¹è±¡
    
    Returns:
        log_mel: np.ndarray, Melé¢‘è°±å›¾ [n_mels, time_frames]
    """
    y = audio.astype(np.float32)
    
    # Step 1: æˆªå–æˆ–è¡¥é›¶åˆ°ç›®æ ‡é•¿åº¦
    target_len = int(config.SAMPLE_RATE * config.WINDOW_DURATION)
    if len(y) > target_len:
        y = y[:target_len]
    else:
        y = np.pad(y, (0, target_len - len(y)), mode='constant')
    
    # Step 2: é«˜é¢‘é¢„åŠ é‡ (ä¸ MEL_preprocess.py ä¸€è‡´)
    if config.PREEMPHASIS:
        y = librosa.effects.preemphasis(y)
    
    # Step 3: è®¡ç®—Melé¢‘è°±å›¾ (å‚æ•°ä¸ MEL_preprocess.py å®Œå…¨ä¸€è‡´)
    mel_spec = librosa.feature.melspectrogram(
        y=y,
        sr=config.SAMPLE_RATE,
        n_mels=config.N_MELS,
        n_fft=config.N_FFT,
        hop_length=config.HOP_LENGTH,
        win_length=config.WIN_LENGTH,
        window=config.WINDOW,
        power=config.POWER
    )
    
    # Step 4: è½¬æ¢ä¸ºå¯¹æ•°åˆ»åº¦ (ä¸ MEL_preprocess.py ä¸€è‡´)
    log_mel = librosa.power_to_db(mel_spec, ref=np.max)
    
    # Step 5: ç¬¬ä¸€æ¬¡æ ‡å‡†åŒ– (ä¸ MEL_preprocess.py ä¸€è‡´)
    if config.NORMALIZE:
        log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-8)
    
    return log_mel


def normalize_for_inference(feat):
    """
    æ¨ç†å‰çš„æ ‡å‡†åŒ– (ä¸ train_MLT.py ä¸­ __getitem__ ä¸€è‡´)
    è®­ç»ƒæ—¶åŠ è½½ .npy æ–‡ä»¶åä¼šå†åšä¸€æ¬¡æ ‡å‡†åŒ–
    
    Args:
        feat: np.ndarray, Melé¢‘è°±ç‰¹å¾
    
    Returns:
        normalized_feat: np.ndarray
    """
    feat = (feat - feat.mean()) / (feat.std() + 1e-6)
    return feat


# =====================
# æ¨ç†åç«¯åŸºç±»
# =====================
class InferenceBackend:
    """æ¨ç†åç«¯åŸºç±»"""
    def __init__(self, model_path, config):
        self.model_path = model_path
        self.config = config
    
    def predict(self, mel_input):
        """
        æ‰§è¡Œæ¨ç†
        Args:
            mel_input: np.ndarray, shape [1, 1, n_mels, time_frames]
        Returns:
            snore_logits: np.ndarray, shape [1, 2]
            posture_logits: np.ndarray, shape [1, 5]
        """
        raise NotImplementedError
    
    def release(self):
        """é‡Šæ”¾èµ„æº"""
        pass


# =====================
# PyTorch CPU åç«¯
# =====================
class PyTorchBackend(InferenceBackend):
    """PyTorch CPU æ¨ç†åç«¯"""
    
    def __init__(self, model_path, config):
        super().__init__(model_path, config)
        import torch
        from model_vo import CNN_TCN_MTL
        
        self.torch = torch
        self.device = torch.device("cpu")
        
        print(f"[PyTorch] Loading model from {model_path}...")
        self.model = CNN_TCN_MTL(
            snore_classes=config.SNORE_CLASSES,
            posture_classes=config.POSTURE_CLASSES
        )
        
        state_dict = torch.load(model_path, map_location=self.device, weights_only=True)
        self.model.load_state_dict(state_dict)
        self.model.to(self.device)
        self.model.eval()
        
        print("[PyTorch] Model loaded successfully (CPU)")
    
    def predict(self, mel_input):
        mel_tensor = self.torch.tensor(mel_input, dtype=self.torch.float32)
        mel_tensor = mel_tensor.to(self.device)
        
        with self.torch.no_grad():
            snore_logits, posture_logits = self.model(mel_tensor)
            return snore_logits.cpu().numpy(), posture_logits.cpu().numpy()


# =====================
# RKNN NPU åç«¯ (RK3588)
# =====================
class RKNNBackend(InferenceBackend):
    """RKNN NPU æ¨ç†åç«¯ (RK3588 åŠ é€Ÿ)"""
    
    def __init__(self, model_path, config):
        super().__init__(model_path, config)
        
        try:
            from rknnlite.api import RKNNLite
        except ImportError:
            raise ImportError(
                "RKNN-Toolkit-Lite2 æœªå®‰è£…!\n"
                "è¯·åœ¨ Orange Pi ä¸Šå®‰è£…: pip3 install rknn-toolkit-lite2\n"
                "æˆ–ä» https://github.com/rockchip-linux/rknn-toolkit2 è·å–"
            )
        
        print(f"[RKNN] Loading model from {model_path}...")
        self.rknn = RKNNLite()
        
        # åŠ è½½ RKNN æ¨¡å‹
        ret = self.rknn.load_rknn(model_path)
        if ret != 0:
            raise RuntimeError(f"Failed to load RKNN model: {ret}")
        
        # åˆå§‹åŒ–è¿è¡Œæ—¶ç¯å¢ƒ
        ret = self.rknn.init_runtime(core_mask=RKNNLite.NPU_CORE_0_1_2)  # ä½¿ç”¨å…¨éƒ¨3ä¸ªNPUæ ¸å¿ƒ
        if ret != 0:
            raise RuntimeError(f"Failed to init RKNN runtime: {ret}")
        
        print("[RKNN] Model loaded successfully (NPU: RK3588)")
    
    def predict(self, mel_input):
        # RKNN è¾“å…¥éœ€è¦æ˜¯ numpy array
        # è¾“å…¥å½¢çŠ¶: [1, 1, 80, 301] (NCHW)
        mel_input = mel_input.astype(np.float32)
        
        # RKNN æ¨ç†
        outputs = self.rknn.inference(inputs=[mel_input])
        
        # outputs[0]: snore_logits [1, 2]
        # outputs[1]: posture_logits [1, 5]
        snore_logits = outputs[0]
        posture_logits = outputs[1]
        
        return snore_logits, posture_logits
    
    def release(self):
        if hasattr(self, 'rknn'):
            self.rknn.release()
            print("[RKNN] Resources released")


# =====================
# å®æ—¶æ¨ç†å™¨
# =====================
class RealtimeSnoreDetector:
    """å®æ—¶æ‰“é¼¾æ£€æµ‹å™¨"""
    
    def __init__(self, model_path, backend="pytorch", config=Config):
        self.config = config
        self.backend_type = backend
        
        # åˆå§‹åŒ–æ¨ç†åç«¯
        if backend == "rknn":
            self.backend = RKNNBackend(model_path, config)
        else:
            self.backend = PyTorchBackend(model_path, config)
        
        # éŸ³é¢‘ç¼“å†²åŒº (å­˜å‚¨3ç§’çš„éŸ³é¢‘)
        self.buffer_size = int(config.SAMPLE_RATE * config.WINDOW_DURATION)
        self.audio_buffer = np.zeros(self.buffer_size, dtype=np.float32)
        self.buffer_lock = threading.Lock()
        
        # æ§åˆ¶æ ‡å¿—
        self.is_running = False
        self.stream = None
        
        # ç»“æœé˜Ÿåˆ—
        self.result_queue = queue.Queue()
        
        # ç¡å§¿æ ‡ç­¾ (ä¸è®­ç»ƒæ—¶çš„æ˜ å°„ä¸€è‡´)
        self.posture_labels = {
            0: "ä»°å§ (Supine)",
            1: "ä»°å§å¤´åå·¦ (Supine, left lateral head)",
            2: "ä»°å§å¤´åå³ (Supine, right lateral head)",
            3: "å·¦ä¾§å§ (Left-side lying)",
            4: "å³ä¾§å§ (Right-side lying)"
        }
    
    def _audio_callback(self, indata, frames, time_info, status):
        """éŸ³é¢‘æµå›è°ƒå‡½æ•°"""
        if status:
            print(f"[WARN] Audio status: {status}")
        
        # è·å–å•å£°é“æ•°æ®
        audio_data = indata[:, 0] if indata.ndim > 1 else indata.flatten()
        
        with self.buffer_lock:
            # æ»‘åŠ¨ç¼“å†²åŒºï¼šç§»é™¤æ—§æ•°æ®ï¼Œæ·»åŠ æ–°æ•°æ®
            shift_len = len(audio_data)
            self.audio_buffer[:-shift_len] = self.audio_buffer[shift_len:]
            self.audio_buffer[-shift_len:] = audio_data
    
    def _inference_loop(self):
        """æ¨ç†å¾ªç¯"""
        interval = self.config.SLIDE_INTERVAL
        
        while self.is_running:
            start_time = time.time()
            
            # è·å–å½“å‰ç¼“å†²åŒºçš„éŸ³é¢‘
            with self.buffer_lock:
                audio = self.audio_buffer.copy()
            
            # æ‰§è¡Œæ¨ç†
            result = self._predict(audio)
            
            # è®°å½•æ¨ç†æ—¶é—´
            inference_time = time.time() - start_time
            result["inference_time_ms"] = inference_time * 1000
            
            # è¾“å‡ºç»“æœ
            self._print_result(result)
            
            # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—
            self.result_queue.put(result)
            
            # ç­‰å¾…åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´çª—å£
            elapsed = time.time() - start_time
            sleep_time = interval - elapsed
            if sleep_time > 0:
                time.sleep(sleep_time)
    
    def _predict(self, audio):
        """
        æ‰§è¡Œå•æ¬¡é¢„æµ‹
        
        å¤„ç†æµç¨‹:
        1. extract_mel_spectrogram: éŸ³é¢‘ -> Melé¢‘è°± (åŒ…å«é¢„åŠ é‡ã€æ ‡å‡†åŒ–)
        2. normalize_for_inference: æ¨ç†å‰å†æ¬¡æ ‡å‡†åŒ– (ä¸è®­ç»ƒæ—¶ä¸€è‡´)
        3. æ¨¡å‹æ¨ç†
        """
        # Step 1: æå–Melé¢‘è°± (ä¸ MEL_preprocess.py ä¸€è‡´)
        mel = extract_mel_spectrogram(audio, self.config)
        
        # Step 2: æ¨ç†å‰æ ‡å‡†åŒ– (ä¸ train_MLT.py __getitem__ ä¸€è‡´)
        mel = normalize_for_inference(mel)
        
        # Step 3: è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ [1, 1, F, T]
        mel_input = mel[np.newaxis, np.newaxis, :, :]
        
        # Step 4: æ¨¡å‹æ¨ç†
        snore_logits, posture_logits = self.backend.predict(mel_input)
        
        # Step 5: è®¡ç®—æ¦‚ç‡ (softmax)
        snore_probs = self._softmax(snore_logits[0])
        posture_probs = self._softmax(posture_logits[0])
        
        # è·å–é¢„æµ‹ç»“æœ
        snore_pred = np.argmax(snore_probs)
        posture_pred = np.argmax(posture_probs)
        
        return {
            "timestamp": datetime.now().strftime("%H:%M:%S"),
            "is_snoring": snore_pred == 1,
            "snore_confidence": float(snore_probs[1]),  # P(snoring)
            "snore_probs": snore_probs,                 # [P(non-snore), P(snore)]
            "posture_pred": int(posture_pred),
            "posture_probs": posture_probs              # 5ç±»ç¡å§¿æ¦‚ç‡
        }
    
    @staticmethod
    def _softmax(x):
        """è®¡ç®— softmax"""
        e_x = np.exp(x - np.max(x))
        return e_x / e_x.sum()
    
    def _print_result(self, result):
        """æ‰“å°æ£€æµ‹ç»“æœ"""
        timestamp = result["timestamp"]
        is_snoring = result["is_snoring"]
        snore_conf = result["snore_confidence"]
        inference_ms = result.get("inference_time_ms", 0)
        
        # æ¸…ç©ºå½“å‰è¡Œå¹¶æ‰“å°
        print("\r" + " " * 140, end="\r")
        
        if is_snoring:
            posture_probs = result["posture_probs"]
            posture_pred = result["posture_pred"]
            posture_label = self.posture_labels.get(posture_pred, f"Unknown({posture_pred})")
            
            # æ ¼å¼åŒ–ç¡å§¿æ¦‚ç‡å‘é‡
            probs_str = ", ".join([f"{p:.2f}" for p in posture_probs])
            
            print(f"[{timestamp}] ğŸ”´ æ‰“é¼¾ ({snore_conf:.0%}) | "
                  f"{posture_label} | "
                  f"[{probs_str}] | {inference_ms:.0f}ms")
        else:
            print(f"[{timestamp}] ğŸŸ¢ æ­£å¸¸ ({1-snore_conf:.0%}) | {inference_ms:.0f}ms", end="")
    
    def start(self, device_id=None):
        """å¼€å§‹å®æ—¶æ£€æµ‹"""
        if self.is_running:
            print("[WARN] Detector is already running")
            return
        
        self.is_running = True
        
        # é…ç½®éŸ³é¢‘æµå‚æ•°
        stream_params = {
            "samplerate": self.config.SAMPLE_RATE,
            "channels": 1,
            "dtype": np.float32,
            "blocksize": int(self.config.SAMPLE_RATE * 0.1),  # 100mså—
            "callback": self._audio_callback
        }
        
        if device_id is not None:
            stream_params["device"] = device_id
        
        # å¯åŠ¨éŸ³é¢‘æµ
        print(f"[INFO] Starting audio stream (SR={self.config.SAMPLE_RATE}Hz)...")
        self.stream = sd.InputStream(**stream_params)
        self.stream.start()
        
        # ç­‰å¾…ç¼“å†²åŒºå¡«æ»¡
        print(f"[INFO] Filling buffer ({self.config.WINDOW_DURATION}s)...")
        time.sleep(self.config.WINDOW_DURATION)
        
        # å¯åŠ¨æ¨ç†çº¿ç¨‹
        print("[INFO] Starting inference loop...")
        print("=" * 80)
        print(f"Real-time Snore Detection | Backend: {self.backend_type.upper()}")
        print(f"Window: {self.config.WINDOW_DURATION}s | Interval: {self.config.SLIDE_INTERVAL}s")
        print("Press Ctrl+C to stop")
        print("=" * 80)
        
        self.inference_thread = threading.Thread(target=self._inference_loop, daemon=True)
        self.inference_thread.start()
    
    def stop(self):
        """åœæ­¢æ£€æµ‹"""
        self.is_running = False
        
        if self.stream is not None:
            self.stream.stop()
            self.stream.close()
            self.stream = None
        
        # é‡Šæ”¾åç«¯èµ„æº
        self.backend.release()
        
        print("\n[INFO] Detector stopped")
    
    def get_latest_result(self, timeout=None):
        """è·å–æœ€æ–°çš„æ£€æµ‹ç»“æœ (ä¾›å¤–éƒ¨è°ƒç”¨)"""
        try:
            return self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return None


# =====================
# å·¥å…·å‡½æ•°
# =====================
def list_audio_devices():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„éŸ³é¢‘è®¾å¤‡"""
    print("\n" + "=" * 70)
    print("Available Audio Devices | å¯ç”¨éŸ³é¢‘è®¾å¤‡")
    print("=" * 70)
    
    devices = sd.query_devices()
    
    # Orange Pi 5 Plus å¸¸è§çš„éŸ³é¢‘è®¾å¤‡åç§°
    orangepi_hints = ["es8388", "rockchip", "hdmi", "analog", "headphone"]
    
    for i, device in enumerate(devices):
        in_ch = device['max_input_channels']
        
        if in_ch > 0:  # åªæ˜¾ç¤ºæœ‰è¾“å…¥çš„è®¾å¤‡
            default_marker = ""
            hint_marker = ""
            
            if i == sd.default.device[0]:
                default_marker = " [DEFAULT]"
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ Orange Pi å¸¸è§è®¾å¤‡
            name_lower = device['name'].lower()
            for hint in orangepi_hints:
                if hint in name_lower:
                    hint_marker = " [Orange Pi]"
                    break
            
            print(f"[{i}] {device['name']}{default_marker}{hint_marker}")
            print(f"    Inputs: {in_ch}ch, Rate: {device['default_samplerate']:.0f}Hz")
    
    print("=" * 70)
    print("\næç¤º: Orange Pi 5 Plus æ¿è½½ 3.5mm éº¦å…‹é£é€šå¸¸æ˜¯ es8388 æˆ– analog è®¾å¤‡")
    print("ä½¿ç”¨ --audio_device <ID> æŒ‡å®šè®¾å¤‡ï¼Œä¾‹å¦‚: --audio_device 0")


def test_audio_device(device_id=None, duration=2):
    """æµ‹è¯•éŸ³é¢‘è®¾å¤‡"""
    print(f"\n[TEST] Recording {duration}s of audio...")
    
    if device_id is not None:
        print(f"[TEST] Using device ID: {device_id}")
    
    try:
        recording = sd.rec(
            int(duration * Config.SAMPLE_RATE),
            samplerate=Config.SAMPLE_RATE,
            channels=1,
            dtype=np.float32,
            device=device_id
        )
        sd.wait()
        
        # è®¡ç®—éŸ³é‡
        rms = np.sqrt(np.mean(recording ** 2))
        peak = np.max(np.abs(recording))
        
        print(f"[TEST] Recording successful!")
        print(f"       RMS: {rms:.6f}, Peak: {peak:.6f}")
        
        if peak < 0.001:
            print("[WARN] âš ï¸  Audio level is very low!")
            print("       è¯·æ£€æŸ¥:")
            print("       1. éº¦å…‹é£æ˜¯å¦æ­£ç¡®æ’å…¥ 3.5mm æ¥å£")
            print("       2. æ˜¯å¦é€‰æ‹©äº†æ­£ç¡®çš„éŸ³é¢‘è®¾å¤‡ (--audio_device)")
            print("       3. alsamixer ä¸­éº¦å…‹é£æ˜¯å¦å·²å¼€å¯å¹¶è°ƒé«˜éŸ³é‡")
        else:
            print("[OK] âœ… Audio device is working properly!")
        
        return True
    except Exception as e:
        print(f"[ERROR] âŒ Audio test failed: {e}")
        print("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. å®‰è£…éŸ³é¢‘åº“: sudo apt-get install portaudio19-dev python3-pyaudio")
        print("2. æ£€æŸ¥ ALSA é…ç½®: arecord -l")
        print("3. å°è¯•å…¶ä»–è®¾å¤‡ ID: python3 realtime_inference.py --list_devices")
        return False


def test_mel_extraction():
    """æµ‹è¯•Melé¢‘è°±æå–"""
    print("\n[TEST] Testing Mel spectrogram extraction...")
    
    # ç”Ÿæˆæµ‹è¯•éŸ³é¢‘ (3ç§’ç™½å™ªå£°)
    duration = Config.WINDOW_DURATION
    sr = Config.SAMPLE_RATE
    test_audio = np.random.randn(int(sr * duration)).astype(np.float32) * 0.1
    
    # æå–Melé¢‘è°±
    start_time = time.time()
    mel = extract_mel_spectrogram(test_audio, Config)
    mel_normalized = normalize_for_inference(mel)
    mel_time = (time.time() - start_time) * 1000
    
    print(f"[TEST] Input audio shape: {test_audio.shape}")
    print(f"[TEST] Mel spectrogram shape: {mel.shape}")
    print(f"[TEST] Expected shape: ({Config.N_MELS}, ~{int(sr * duration / Config.HOP_LENGTH) + 1})")
    print(f"[TEST] Mel extraction time: {mel_time:.1f}ms")
    print(f"[TEST] Mel mean: {mel.mean():.4f}, std: {mel.std():.4f}")
    print(f"[TEST] After normalize: mean={mel_normalized.mean():.4f}, std={mel_normalized.std():.4f}")
    print("[OK] âœ… Mel extraction test passed!")


def benchmark_inference(model_path, backend="pytorch", num_runs=10):
    """æ¨ç†æ€§èƒ½æµ‹è¯•"""
    print(f"\n[BENCHMARK] Testing inference performance ({backend})...")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test_audio = np.random.randn(int(Config.SAMPLE_RATE * Config.WINDOW_DURATION)).astype(np.float32) * 0.1
    
    # åˆå§‹åŒ–åç«¯
    if backend == "rknn":
        inference_backend = RKNNBackend(model_path, Config)
    else:
        inference_backend = PyTorchBackend(model_path, Config)
    
    # é¢„çƒ­
    mel = extract_mel_spectrogram(test_audio, Config)
    mel = normalize_for_inference(mel)
    mel_input = mel[np.newaxis, np.newaxis, :, :]
    
    _ = inference_backend.predict(mel_input)
    
    # æµ‹è¯•
    times = []
    for _ in range(num_runs):
        start = time.time()
        _ = inference_backend.predict(mel_input)
        times.append((time.time() - start) * 1000)
    
    inference_backend.release()
    
    print(f"[BENCHMARK] Results ({num_runs} runs):")
    print(f"  Mean: {np.mean(times):.2f}ms")
    print(f"  Min:  {np.min(times):.2f}ms")
    print(f"  Max:  {np.max(times):.2f}ms")
    print(f"  Std:  {np.std(times):.2f}ms")


# =====================
# ä¸»å‡½æ•°
# =====================
def main():
    parser = argparse.ArgumentParser(
        description="Real-time Snore Detection for Orange Pi 5 Plus (RK3588)",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_path", type=str, default="./mlt_best.pth",
                        help="Path to model (.pth for PyTorch, .rknn for RKNN)")
    parser.add_argument("--backend", type=str, choices=["pytorch", "rknn"], default="pytorch",
                        help="Inference backend: pytorch (CPU) or rknn (NPU)")
    
    # éŸ³é¢‘å‚æ•°
    parser.add_argument("--audio_device", type=int, default=None,
                        help="Audio input device ID. Use --list_devices to see available devices")
    parser.add_argument("--sample_rate", type=int, default=16000,
                        help="Audio sample rate")
    parser.add_argument("--window", type=float, default=3.0,
                        help="Window duration in seconds")
    parser.add_argument("--interval", type=float, default=1.0,
                        help="Slide interval in seconds")
    
    # å·¥å…·é€‰é¡¹
    parser.add_argument("--list_devices", action="store_true",
                        help="List available audio devices and exit")
    parser.add_argument("--test_audio", action="store_true",
                        help="Test audio device and exit")
    parser.add_argument("--test_mel", action="store_true",
                        help="Test Mel spectrogram extraction and exit")
    parser.add_argument("--benchmark", action="store_true",
                        help="Run inference benchmark and exit")
    
    args = parser.parse_args()
    
    # åˆ—å‡ºè®¾å¤‡
    if args.list_devices:
        list_audio_devices()
        return
    
    # æµ‹è¯•Melæå–
    if args.test_mel:
        test_mel_extraction()
        return
    
    # æµ‹è¯•éŸ³é¢‘
    if args.test_audio:
        list_audio_devices()
        test_audio_device(args.audio_device)
        return
    
    # æ€§èƒ½æµ‹è¯•
    if args.benchmark:
        if not os.path.exists(args.model_path):
            print(f"[ERROR] Model file not found: {args.model_path}")
            sys.exit(1)
        benchmark_inference(args.model_path, args.backend)
        return
    
    # æ›´æ–°é…ç½®
    Config.SAMPLE_RATE = args.sample_rate
    Config.WINDOW_DURATION = args.window
    Config.SLIDE_INTERVAL = args.interval
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(args.model_path):
        print(f"[ERROR] Model file not found: {args.model_path}")
        sys.exit(1)
    
    # è‡ªåŠ¨æ£€æµ‹åç«¯
    if args.model_path.endswith('.rknn'):
        args.backend = "rknn"
        print("[INFO] Detected RKNN model, using NPU backend")
    elif args.model_path.endswith('.pth'):
        args.backend = "pytorch"
        print("[INFO] Detected PyTorch model, using CPU backend")
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = RealtimeSnoreDetector(
        model_path=args.model_path,
        backend=args.backend,
        config=Config
    )
    
    try:
        # å¯åŠ¨æ£€æµ‹
        detector.start(device_id=args.audio_device)
        
        # ä¸»å¾ªç¯
        while True:
            time.sleep(1)
            
    except KeyboardInterrupt:
        print("\n[INFO] Interrupted by user")
    finally:
        detector.stop()


if __name__ == "__main__":
    main()
